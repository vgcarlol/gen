# lexer_generator.py
import os

def generar_lexer_py(afd, token_map, ruta_salida="output/lexer.py"):
    os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)
    with open(ruta_salida, "w", encoding="utf-8") as f:
        # Cabecera y definición de la función analizar
        f.write("# AUTO-GENERADO: Analizador léxico basado en AFD\n\n")
        f.write("def analizar(texto):\n")
        # Función auxiliar para validar números sin usar re
        f.write("    def es_numero(s):\n")
        f.write("        if not s:\n")
        f.write("            return False\n")
        f.write("        i = 0\n")
        f.write("        n = len(s)\n")
        f.write("        # Debe tener al menos un dígito\n")
        f.write("        if i < n and s[i].isdigit():\n")
        f.write("            while i < n and s[i].isdigit():\n")
        f.write("                i += 1\n")
        f.write("        else:\n")
        f.write("            return False\n")
        f.write("        # Parte decimal opcional\n")
        f.write("        if i < n and s[i] == '.':\n")
        f.write("            i += 1\n")
        f.write("            if i >= n or not s[i].isdigit():\n")
        f.write("                return False\n")
        f.write("            while i < n and s[i].isdigit():\n")
        f.write("                i += 1\n")
        f.write("        # Parte exponencial opcional\n")
        f.write("        if i < n and (s[i] == 'E' or s[i] == 'e'):\n")
        f.write("            i += 1\n")
        f.write("            if i < n and (s[i] == '+' or s[i] == '-'):\n")
        f.write("                i += 1\n")
        f.write("            if i >= n or not s[i].isdigit():\n")
        f.write("                return False\n")
        f.write("            while i < n and s[i].isdigit():\n")
        f.write("                i += 1\n")
        f.write("        return i == n\n\n")
        # Transiciones obtenidas del AFD
        f.write("    transitions = {\n")
        for (estado, simbolo), destino in afd.getTransitions().items():
            estado_str = repr(estado)
            simbolo_str = repr(simbolo)
            destino_str = repr(destino)
            f.write(f"        ({estado_str}, {simbolo_str}): {destino_str},\n")
        f.write("    }\n\n")
        # Diccionario de estados aceptantes
        f.write("    accepting = {\n")
        if hasattr(afd, "accepting_map"):
            for estado, (_, token_name) in afd.accepting_map.items():
                f.write(f"        {repr(estado)}: {repr(token_name)},\n")
        f.write("    }\n\n")
        # Estado inicial
        f.write("    estado_inicial = " + repr(afd.getStart()) + "\n\n")
        # Inicio del análisis léxico
        f.write("    tokens = []\n")
        f.write("    i = 0\n")
        f.write("    while i < len(texto):\n")
        f.write("        estado_actual = estado_inicial\n")
        f.write("        lexema = ''\n")
        f.write("        ultimo_token = None\n")
        f.write("        ultimo_index = i\n")
        f.write("        j = i\n")
        f.write("        while j < len(texto):\n")
        f.write("            c = texto[j]\n")
        f.write("            if (estado_actual, c) in transitions:\n")
        f.write("                estado_actual = transitions[(estado_actual, c)]\n")
        f.write("                lexema += c\n")
        f.write("                if estado_actual in accepting:\n")
        f.write("                    ultimo_token = (lexema, accepting[estado_actual])\n")
        f.write("                    ultimo_index = j + 1\n")
        f.write("                j += 1\n")
        f.write("            else:\n")
        f.write("                break\n")
        f.write("        if ultimo_token:\n")
        f.write("            tokens.append(ultimo_token)\n")
        f.write("            i = ultimo_index\n")
        f.write("        else:\n")
        f.write("            tokens.append((texto[i], 'ERROR'))\n")
        f.write("            i += 1\n\n")
        # Postprocesamiento: validación de tokens 'ID'
        f.write("    tokens_corr = []\n")
        f.write("    # Se verifica si 'NUMBER' está definido en el diccionario de tokens aceptantes\n")
        f.write("    number_definido = any(tok == 'NUMBER' for tok in accepting.values())\n")
        f.write("    for lex, tok in tokens:\n")
        f.write("        # Si el token es 'ID' pero no comienza con una letra\n")
        f.write("        if tok == 'ID' and (not lex or lex[0] not in 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'):\n")
        f.write("            if number_definido:\n")
        f.write("                if es_numero(lex):\n")
        f.write("                    tokens_corr.append((lex, 'NUMBER'))\n")
        f.write("                else:\n")
        f.write("                    tokens_corr.append((lex, 'ERROR'))\n")
        f.write("            else:\n")
        f.write("                tokens_corr.append((lex, 'ERROR'))\n")
        f.write("        else:\n")
        f.write("            tokens_corr.append((lex, tok))\n")
        f.write("    return tokens_corr\n\n")
        # Bloque main para pruebas
        f.write("if __name__ == '__main__':\n")
        f.write("    with open('input/random_data_3.txt', 'r', encoding='utf-8') as file:\n")
        f.write("        contenido = file.read()\n")
        f.write("        resultado = analizar(contenido)\n\n")
        f.write("    with open('output/tokens.txt', 'w', encoding='utf-8') as out:\n")
        f.write("        for lexema, token in resultado:\n")
        f.write("            out.write(f\"{lexema} -> {token}\\n\")\n")
    print(f"✅ lexer.py generado en: {ruta_salida}")
